---
title: "SOGI IWG: Evaluation of Research to Date (Statistical Topic Modeling) "
author: "Pri Oberoi"
date: "July 21, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Header

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.


```{r cars}
#load text mining library
library(tm)
library(SnowballC)
library(topicmodels)


#set working directory (modify path as needed)
setwd("/Users/prioberoi/Documents/pri_personal/sogi_iwg/sogi/data/")
path <- "/Users/prioberoi/Documents/pri_personal/sogi_iwg/sogi"

#load files into corpus
#get listing of .txt files in directory
filenames <- list.files(getwd(),pattern="*.txt")


#read files into a character vector
files <- lapply(filenames,readLines)


#create corpus from vector
docs <- Corpus(VectorSource(files))


#inspect a particular document in corpus
writeLines(as.character(docs[[30]]))


#start preprocessing

#Convert to UTF-8 format
docs <- tm_map(docs, content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')))

#Transform to lower case
docs <-tm_map(docs,content_transformer(tolower), lazy = TRUE)


#remove potentially problematic symbols
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
docs <- tm_map(docs, toSpace, "-", lazy = TRUE)
docs <- tm_map(docs, toSpace, "’", lazy = TRUE)
docs <- tm_map(docs, toSpace, "‘", lazy = TRUE)
docs <- tm_map(docs, toSpace, "•", lazy = TRUE)
docs <- tm_map(docs, toSpace, "“", lazy = TRUE)
docs <- tm_map(docs, toSpace, "”", lazy = TRUE)

#remove punctuation
docs <- tm_map(docs, removePunctuation, lazy = TRUE)
#Strip digits
docs <- tm_map(docs, removeNumbers, lazy = TRUE)
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"), lazy = TRUE)
#remove whitespace
docs <- tm_map(docs, stripWhitespace, lazy = TRUE)
#Good practice to check every now and then
writeLines(as.character(docs[[1]]))
#Stem document <--------- look into Lemmatization instead
docs <- tm_map(docs,stemDocument)


#fix up 1) differences between us and aussie english 2) general errors
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "organiz", replacement = "organ")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "organis", replacement = "organ")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "andgovern", replacement = "govern")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "inenterpris", replacement = "enterpris")
docs <- tm_map(docs, content_transformer(gsub),
               pattern = "team-", replacement = "team")

#define and eliminate all custom stopwords
myStopwords <- c("can", "say","one","way","use",
                  "also","howev","tell","will",
                  "much","need","take","tend","even",
                  "like","particular","rather","said",
                  "get","well","make","ask","come","end",
                  "first","two","help","often","may",
                  "might","see","someth","thing","point",
                  "post","look","right","now","think","‘ve ",
                  "‘re ","anoth","put","set","new","good",
                  "want","sure","kind","larg","yes,","day","etc",
                  "quit","sinc","attempt","lack","seen","awar",
                  "littl","ever","moreov","though","found","abl",
                  "enough","far","earli","away","achiev","draw",
                  "last","never","brief","bit","entir","brief",
                  "great","lot")
docs <- tm_map(docs, removeWords, myStopwords)
#inspect a document as a check
writeLines(as.character(docs[[1]]))


#Create document-term matrix
dtm <- DocumentTermMatrix(docs, control=list(bounds = list(global = c(2, Inf)))) # created dtm excluding terms that only appear in 1 doc
#convert rownames to filenames
rownames(dtm) <- filenames
#collapse matrix by summing over columns
freq <- colSums(as.matrix(dtm))
inspect(dtm) <- inspect(dtm)[,12:ncol(inspect(dtm))]
#inspect(dtm[1:5,1:5])
#length should be total number of terms
length(freq)
#create sort order (descending)
ord <- order(freq,decreasing=TRUE)
#List all terms in decreasing order of freq and write to disk
freq[head(ord)]
write.csv(freq[ord],"/Users/prioberoi/Documents/pri_personal/sogi_iwg/sogi/word_freq.csv")


###########
# Topic modeling using LDA
###########

#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 5

#Run LDA using Gibbs sampling
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed=seed, best=best, burnin=burnin, iter=iter, thin=thin))


#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste0(path, "/LDAGibbs",k,"DocsToTopics.csv"))


#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,6))
write.csv(ldaOut.terms,file=paste0(path, "/LDAGibbs",k,"TopicsToTerms.csv"))


#probabilities associated with each topic assignment
# this lists the probabilities with which each topic is assigned to a document. 
# This is therefore a 30 x 5 matrix – 30 docs and 5 topics. 
# As one might expect, the highest probability in each row corresponds to the topic assigned to that document.  
# The “goodness” of the primary assignment can be assessed by taking the ratio of the highest to second-highest probability 
# and the second-highest to the third-highest probability and so on. 
# Which is what I’ve done in the lines after this one.
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste0(path, "/LDAGibbs",k,"TopicProbabilities.csv"))


#Find relative importance of top 2 topics
topic1ToTopic2 <- lapply(1:nrow(dtm),function(x)
  sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1])


#Find relative importance of second and third most important topics
topic2ToTopic3 <- lapply(1:nrow(dtm),function(x)
  sort(topicProbabilities[x,])[k-1]/sort(topicProbabilities[x,])[k-2])


#write to file
write.csv(topic1ToTopic2,file=paste0(path, "/LDAGibbs",k,"Topic1ToTopic2.csv"))
write.csv(topic2ToTopic3,file=paste0(path, "/LDAGibbs",k,"Topic2ToTopic3.csv"))

```
